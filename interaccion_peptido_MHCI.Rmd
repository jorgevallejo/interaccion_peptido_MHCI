---
title: "Predicción de interacción entre péptido y el complejo mayor de histocompatibilidad tipo I con Artificial Neural Networks (ANN) y Support Vector Machines (SVM)"
subtitle: "Machine Learning - PEC 2"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%e de %B, %Y")`'
output:
  pdf_document:
    number_sections: true
header-includes:
  - \renewcommand{\contentsname}{Sumario}
toc: true
# Next code for knitting more than one type of document automatically comes from https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
  output_format = "all",
  output_dir = "results") })
# And:
# https://stackoverflow.com/a/46007686/10647267

bibliography: scholar.bib
---

```{r setup, include=FALSE}
# knitr options
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE)

# This is a try:
knitr::opts_knit$set(stop_on_error = 2L)
# See ?evaluate::evaluate
# What I am trying to do is to make knitr stop
# when an error is found instead of running the
# complete script.
```

```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(ggplot2)
library(ggseqlogo)
library(neuralnet)
library(RSNNS)
```

```{r create directory structure}
directories <- c("data", "results", "intermediateData")

# Create directories
for (i in directories){
  if (!(dir.exists(i))){
dir.create(i)
}
}
```

```{r delete results files, eval= FALSE}
# Run this chunk ONLY if you want to re-do
# all the report FROM ZERO.
# Remember that the .RData files are there to
# avoid unnecesarily redoing long data processing.

file.remove(c("intermediateData/model.RData",
              "intermediateData/model2.RData",
              "intermediateData/encoded_sequences.RData",
              "results/interaccion_peptido_MHCI.pdf"))
```
\newpage

# Algoritmo Red Neuronal Artificial

Los algoritmos de Red Neuronal Artificial (o _ANN_ por el inglés _'Artificial Neuronal Net'_), son simulaciones que imitan el funcionamiento de las redes neuronales biológicas. Por lo general no modelan redes neuronales concretas, sino modelos abstractos.

Como abstracción de una red neural biológica, se podría decir que las ANNs constan de dos tipos de "piezas": **nodos** y **vértices**.

- Los **nodos** de las ANNs son objetos de código que - igual que hacen las neuronas biológicas - aceptan datos, los procesan, y envían datos a su vez.

- Los **vértices** son el equivalente a los axones y dendritas. Definen qué neuronas están conectadas entre ellas y, por tanto, cómo viaja la información dentro de la red neuronal.

Cada ANN se puede describir por las siguientes características:  
- La **función de activación**.  
- La **topología**.  
- El **algoritmo de entrenamiento**.

La **función de activación** describe la forma en que los nodos combinan los datos que les llegan para generar los datos que envían. Diferentes funciones de activación son más adecuadas para diferentes tareas de aprendizaje. Las funciones más comunes son la _función de paso único_, la _lineal_, la _lineal saturada_, la _tangente hiperbólica_ y la _gaussiana_.

La **topología** de la red describe el número de nodos por capa, el número de capas, y cómo se conectan unas con otras (número de nodos y si la información puede viajar "hacia atrás", de capas posteriores a capas anteriores).

El **algoritmo de entrenamiento** configura la forma en que se ponderan los datos que se transmiten por cada vértice, una forma de modular las señales que viajan por la red _adicional a la función de activación_.

## Fortalezas y debilidades de las Redes Neuronales Artificiales

|Fortalezas|Debilidades|
|----------------------------------------|----------------------------------------|
| - Se pueden usar tanto para problemas de clasificación como para predicción numérica. | - Necesitan enormes cantidades de recursos de cálculo y su entrenamiento es lento, especialmente cuando la topología de la red es compleja|
|||
| - Pueden modelar patrones más complejos que casi cualquier otro algoritmo. | - Muy susceptibles a sobreajustar los datos de entrenamiento. |
|||
| - Hace pocas asunciones previas acerca de las relaciones entre los datos. | - El modelo resultante del entrenamiento es de tipo caja negra, complejo y difícil (si no imposible) de interpretar. |

# Algoritmo Support Vector Machine

Los algoritmos SVM (_Support Vector Machine_) procesan datos distribuidos en un espacio multidimensional, y los utilizan para definir una superficie (hiperplano) que divide dicho espacio en regiones homogéneas que agrupan observaciones afines. Estos algoritmos se usan en el campo del aprendizaje automático tanto para clasificar observaciones, como para hacer predicciones. Este pequeño script pondrá en aplicación una de sus funciones más sencillas, la clasificación de un conjunto de observaciones en dos grupos diferenciados.

Cuando las observaciones de ambos grupos están claramente separadas y pueden separarse completamente mediante una línea, plano, o su equivalente multidimensional (hiperplano), hablamos de **datos separables linealmente**. Sin embargo, en la vida real es muy común que la relación entre variables no sea lineal. Es estos casos, se pueden seguir dos estrategias: aplicación de la _slack variable_ (variable fluctuante) y el uso del _kernel trick_ (cambio de la función kernel).

El caso de la variable _slack_ consiste en añadir una variable el algoritmo que "permite" dejar observaciones de entrenamiento fuera del grupo al que propiamente pertenecen. Aquí lo que produce el algoritmo es una optimización del hiperplano para miniminar la distancia de esas observaciones hasta el límite que define su clasificación correcta.

El _kernel trick_ es matemáticamente más complejo y consiste en añadir nuevas dimensiones a los datos y usarlas para identificar las relaciones no lineales entre las variables. Aquí las superficies que definen los límites entre grupos ya no son hiperplanos, sino curvas de mayor o menor complejidad.

## Fortalezas y debilidades del algoritmo SVM

|Fortalezas|Debilidades|
|----------------------------------------|----------------------------------------|
| - Se puede usar tanto para problemas de clasificación como para predicción numérica. | - Encontrar el modelo más adecuado requiere probar varias combinaciones de kernels y parámetros. |
|||
| - Poco sensible al ruido en los datos y poco propenso al sobreajuste (overfitting). | - El entrenamiento puede ser lento, en especial se el set de datos tiene un gran número de características o ejemplos. |
|||
| - Puede ser de más fácil uso que las redes neurales, especialmente debido a la existencia de varios algoritmos SVM que han sido adaptados para su uso en ciencia de datos. | - El modelo resultante del entrenamiento es de tipo caja negra, complejo y difícil (si no imposible) de interpretar. |
|||
| - Popular debido a su alta precisión y a la fama debida a su uso ganador en competiciones de minería de datos. | |

# Análisis exploratorio

```{r check the existence of data file}
# The file with the dataset must be in a directory called "data"
# placed in the same directory of the code we are going to run

if (! dir.exists("./data")) {
  stop("El directorio ./data no existe.
       El dataset debe estar en el directorio ./data para generar el reporte.")
}

# Check how many files (if any) are in data directory with format csv
csv_files <- list.files(path = "./data", pattern = "csv")

if (length(csv_files) < 1){
  stop("No se ha encontrado en el directorio './data'
       ningún archivo con extensión csv.
       Para el uso de este informe automático es necesario
       que el dataset esté en forma de fichero csv (y con extensión '.csv')
       en el directorio '.data/'")
}else if (length(csv_files) > 1){
  stop("Demasiados archivos con extensión '.csv' en el directorio './data'.
       Para el uso de este informe automático es necesario 
       que el dataset esté en forma de un único fichero csv 
       (y con extensión '.csv') en el directorio '.data/'")
}

```

El set de datos para este informe proviene del fichero "`r csv_files`".

```{r structure of the data}
# Read dataset into a data frame and check its structure
raw_dataframe <- read.csv2(
  file.path("data", csv_files),
  stringsAsFactors = FALSE)

raw_dataframe$label <- as.factor(raw_dataframe$label)

observaciones <- nrow(raw_dataframe)
variables <- ncol(raw_dataframe)

```

El dataset está compuesto por:  
**`r observaciones` observaciones**, de cada una de las cuales se han obtenido  
**`r variables` variables**.

```{r variables}
str(raw_dataframe)
```

La primera variable, 'sequence', corresponde a la secuencia de aminoácidos en cada péptido en la que cada aminoácido está codificado por una letra siguiendo las [recomendaciones de la IUPAC](https://www.ncbi.nlm.nih.gov/Class/MLACourse/Modules/MolBioReview/iupac_aa_abbreviations.html)[@iupac1984iupac].

La segunda variable, 'label', nos informa de la interacción del péptido con el MHCI; donde 'NB' significa que no hay interacción, y 'SB' significa que sí hay interacción.

Un dato que nos interesaría conocer es el patrón que sigue la secuencia de péptidos en cada categoría ('NB' y 'SB'), representarlo como [secuencia logo](https://es.wikipedia.org/wiki/Logo_de_secuencias), y ver si hay alguna diferencia apreciable a simple vista.

```{r modificando las cadenas de la secuencia}
# Compruebo si hay secuencias duplicadas
secuencias_unicas <- length(unique(raw_dataframe$sequence))
# Compruebo si todas las secuencias tienen la misma longitud
secuencias_longitud <- table(nchar(raw_dataframe$sequence))
```
Antes de nada, comprobamos si tenemos secuencias repetidas en nuestros datos:  
- Número de observaciones: **`r format(observaciones, big.mark = " ")`**.  
- Número de secuencias únicas: **`r format(secuencias_unicas, big.mark = " ")`**.

Y la longitud de secuencia de todos los péptidos:
```{r tabla de longitudes de peptidos}
kable(secuencias_longitud,
      col.names = c("Longitud", "Péptidos"),
      align = 'cc',
      format.args = list(big.mark = " "))
```
```{r generate data sets for seqlogos}
# Segregate sequences by label
NB_sequences <- raw_dataframe[raw_dataframe$label == 'NB', ]
SB_sequences <- raw_dataframe[raw_dataframe$label == 'SB', ]
```

- Sequencias sin interacción con el MHCI: **`r format(nrow(NB_sequences), big.mark=" ")`**  
- Sequencias que interaccionan con el MHCI: **`r format(nrow(SB_sequences), big.mark=" ")`** 

Una vez aclarada la estructura de las secuencias pasamos a generar los logos de secuencia; en este caso usando el paquete `ggseqlogo`[@10.1093/bioinformatics/btx469].

```{r generate seqlogos, fig.height=4, fig.width=4, fig.cap='Logo de secuencia de péptidos sin interacción con MHCI (superior) y con interacción (inferior).'}
# Draw the sequence logos
p1 <- ggseqlogo(NB_sequences$sequence, method = 'bit')
p2 <- ggseqlogo(SB_sequences$sequence, method = 'bit')
gridExtra::grid.arrange(p1, p2)
```

Al comparar los logos de secuencia de los péptidos que no muestran interacción con el MHCI (superior), y los que sí interactúan (inferior); llama la atención la prevalencia de aminoácidos hidrofóbicos en las posiciones 2 y 9 de los péptidos con interacción.

# Pre-procesado de datos

## One-shot encoding

Como paso previo al análisis mediante aprendizaje automático, los péptidos han sido recodificados mediante la transformación conocida como [*one hot encoding*](https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f), según la cual cada aminoácido en la secuencia del péptido es representado en código binario.

```{r one-hot enconding}
oneHotEncoding <- function(x) {
  aminoacidos <- c("A", "R", "N", "D", "C", "Q", "E", "G", "H", "I", "L", "K",
                   "M", "F", "P", "S", "T", "W", "Y", "V")
  # Coding function
  coder <- function(x){
    # Make dataframe of zeroes
    coded <- data.frame(matrix(0, ncol = 20, nrow = 9))
    # One column for each of proteingenic aminoacids
    colnames(coded) <- aminoacidos
    # Row with the peptide sequence
    coded$peptido <- unlist(strsplit(x[1], ""))
    
    # Replace zeros with ones when there is aminoacid coincidence
    for (row in 1:(length(coded)-1)){
      coded[, row] <- replace(coded[ ,row], coded$peptido == colnames(coded[row]), 1)
      }
    # Take out peptido row
    coded$peptido <- NULL
    # Transform the data frame into a vector
    return(coded <- as.vector(t(coded)))
  }
  temporal_encoded <- apply(x, 1, coder)
  return(as.data.frame(t(temporal_encoded)))
}
```

```{r encode all aminoacid sequences}
# Check firstly if an already encoded object exists,
# since it takes a lot of time to encode
if(file.exists("intermediateData/encoded_sequences.RData")){
  load("intermediateData/encoded_sequences.RData")
}else{
  encoded_sequences <- oneHotEncoding(raw_dataframe)
  save(encoded_sequences, file="intermediateData/encoded_sequences.RData")
}
```

```{r add labels}
encoded_sequences$label <- raw_dataframe$label
```

Hecho esto, vemos que los datos con los que alimentaremos los algoritmos de aprendizaje automático se componen de **`r format(nrow(encoded_sequences), big.mark=" ")`** secuencias y **`r format(ncol(encoded_sequences), big.mark=" ")`** variables cada secuencia.

La siguiente secuencia es un ejemplo de péptido (`r raw_dataframe[1,1]`) codificado mediante one-hot encoding:
```{r print example of coded peptide}
as.vector(t(encoded_sequences[1, 1:180]))
```


## Sets de entrenamiento y prueba

```{r separa sets entrenamiento y prueba}
# Observations by subset
train <- round((2*observaciones)/3)
test <- observaciones-train

set.seed(123)
# Reordered row numbers
shuffled_rows <- sample(observaciones)
training_rows <- shuffled_rows[1:train]
test_rows <- shuffled_rows[(train + 1):observaciones]
# Subset the dataframe
training_dataframe <- encoded_sequences[training_rows, ]
test_dataframe <- encoded_sequences[test_rows, ]
```

Como último paso antes del análisis, hemos dividido el set de datos - al azar - en un subset de entrenamiento (2/3 de las observaciones), y un subset de evaluación (1/3 de las observaciones).

**Entrenamiento:** `r format(train, big.mark= " ")` observaciones.  
**Evaluación:** `r format(test, big.mark=" ")` observaciones.

# Análisis de los datos con Red Neuronal Artificial

## Primer modelo
```{r train the one node model}
set.seed(1234567)

if(file.exists("intermediateData/model.RData")){
  load("intermediateData/model.RData")
}else{
model <- neuralnet(label ~ ., 
                   data =  training_dataframe,
                   hidden = 1,
                   linear.output = FALSE)
save(model, file = "./intermediateData/model.RData")
}
```

El primer modelo consta de:  
`r length(model$model.list$variables)` nodos de input  
1 capa escondida compuesta por 1 nodo  
`r length(model$model.list$response)` nodos output

Error: `r model$result.matrix[1]`  
Pasos de entrenamiento: `r model$result.matrix[3]`

```{r evaluate model one performance}
model_results <- predict(model, test_dataframe)
model_results <- max.col(model_results)
model_results <- factor(model_results, labels = c("NB", "SB"))
#mean(test_dataframe$label == model_results)

# confusion matrix for linear from caret package
library(caret)
# Set SB as the positive
one_node_matrix <- confusionMatrix(model_results,
                test_dataframe$label,
                positive = "SB")
```

## Segundo modelo
```{r train the three node model}
set.seed(1234567)

if(file.exists("intermediateData/model2.RData")){
  load("intermediateData/model2.RData")
}else{
model2 <- neuralnet(label ~ ., 
                   data =  training_dataframe,
                   linear.output = FALSE,
                   hidden = 3)
save(model2, file = "./intermediateData/model2.RData")
}
```


El segundo modelo consta de:  
`r length(model2$model.list$variables)` nodos de input  
1 capa escondida compuesta por 3 nodos  
`r length(model2$model.list$response)` nodos output

Error: `r model2$result.matrix[1]`  
Pasos de entrenamiento: `r model2$result.matrix[3]`

## Evaluación de los modelos
Para evaluar los modelos utilizamos el set de datos de prueba que habíamos especificado con anterioridad.

```{r evaluate model three performance}
model_results2 <- predict(model2, test_dataframe)
model_results2 <- max.col(model_results2)
model_results2 <- factor(model_results2, labels = c("NB", "SB"))
#mean(test_dataframe$label == model_results2)

# confusion matrix for linear from caret package
# Set SB as the positive
three_node_matrix <- confusionMatrix(model_results2,
                test_dataframe$label,
                positive = "SB")
```

A la hora de evaluar modelos predictivos hay tres parámetros sencillos de calcular y explicar, y son los que se han usado en este informe:

- **Sensibilidad**: Es la proporción de observaciones positivas que han sido clasificadas correctamente.
$$
\text{sensibilidad} = \frac{\text{positivos auténticos}}{\text{positivos auténticos } + \text{ falsos negativos}}
$$

- **Especificidad**: Es la proporción de observaciones negativas que han sido clasificadas correctamente.
$$
\text{especificidad} = \frac{\text{negativos auténticos}}{\text{negativos auténticos } + \text{ falsos positivos}}
$$
- **Precisión**: Es la proporción de ejemplos clasificados como positivos que son _realmente_ positivos.

$$
\text{precisión} = \frac{\text{positivos auténticos}}{\text{positivos auténticos } + \text{ falsos positivos}}
$$

El objetivo del modelo de clasificación que hemos construido es distinguir entre péptidos que interactúan con el MHCI (positivo) y péptidos que no interactúan con el MHCI (negativo).

En la siguiente tabla podemos comparar los tres parámetros de evaluación para ambos modelos (con 1 nodo o con 3 nodos en la capa oculta):

```{r create dataframe with evaluation values, results='asis'}
# Recupera los parametros de interes
one_node_evaluation <- one_node_matrix$byClass[c(1, 2, 5)]
three_node_evaluation <- three_node_matrix$byClass[c(1, 2, 5)]
# Create data frame
evaluation_table <- as.data.frame(rbind(one_node_evaluation,
                          three_node_evaluation))
# Change column and row names
colnames(evaluation_table) <- c("Sensibilidad", "Especificidad", "Precisión")
rownames(evaluation_table) <- c("Un nodo oculto", "Tres nodos ocultos")
```

```{r print table, results="asis"}
# Print table
knitr::kable(evaluation_table, format="markdown",
             digits = 4,
             align = 'ccc')
```

Ambas redes obtienen muy buenos resultados, sin embargo la red con un único nodo en la capa oculta es ligeramente mejor. Quizá se deba a que la red con tres nodos en la capa oculta ha producido un modelo sobreajustado a los datos de entrenamiento, y eso haga que su desempeño sea peor con el set de datos de prueba.

En resumen, aunque ambos modelos son iguales en cuanto a _precisión_, el modelo construido con la red de **un nodo oculto** es ligeramente mejor cuando tenemos en cuenta _especificidad_ y _precisión_.

# ANN con caret
```{r create cross-validation method}
fitControl <- trainControl(
  method = 'boot',
  number = 5)
```

```{r train ANN model}
set.seed(1234567)
if(file.exists("intermediateData/model.ann.caret.RData")){
  load("intermediateData/model.ann.caret.RData")
}else{
model.ann.caret <- caret::train(label ~ .,
             data=training_dataframe,
             size = 3,
             trControl = fitControl,
             method='mlp'
             )
save(model2, file = "./intermediateData/model.ann.caret.RData")
}
```


# Referencias